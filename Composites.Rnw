% !Rnw root = Test.Rnw
\chapter{Composite Scores}
Clinicians use \defword{composite scores}\marginnote{A \defword{composite variable} is the (possibly weighted) sum of two or more variables.} whenever they can because they tend to be more reliable and valid than single scores. How much more reliable is a composite score than a single score? It, of course, depends. First, the reliability coefficients of all of the scores that make up the composite score matter. Second, the correlations among the components of the composite matter. The more reliable and correlated the components, the more reliable the composite.

As discussed previously, it is quite possible for different scores from the same test to have different reliabilities. Scores from the same test tend to have similar reliabilities, though it is quite possible for differences in reliability to be large. In a well designed test, such cases are rare. If we can assume that the test is well designed and that test score reliabilities are within a fairly narrow range, we can assume that the classical reliability coefficient is a good estimate of the reliability of all scores from a particular test. To the degree that such assumptions are incorrect, the following equations will be less accurate. 

Before we can discuss the reliability of composites, we must cover more basic statistics as they apply to composite scores. I will present this material in three ways. First, I will show the equations in standard notation. Adjacent to this, I will show the same equations in matrix notation.\marginpar{Why bother with matrices? It does seem to be an unnecessary complication at first. However, many things become clearer and simpler with matrices, which I hope to illustrate. Furthermore, R was designed to work elegantly with vectors and matrices.} Then I will walk through the calculations using R.

\section{The Mean of the Sum Is the Sum of the Means}

This section is going to make a simple idea look complicated. If you get lost, this is what I am trying to say: If we create a new random variable by adding a bunch of random variables together, the mean of that new variable is found by adding together the means of all the variables we started with. I'm sorry for what comes next but the work we put into it will pay off later. Okay, now let's make that simple idea formal and hard to understand: 

We can start with the example of two variables: $X_1$ and $X_2$. The sum of these variables, which we will call $X_S$, has a mean, which is the sum of the means of $X_1$ and $X_2$. That is, $\mu_{S} = \mu_{1} + \mu_{2}$. 

What if we have three variables? or four? or five? It would be tedious to illustrate each case one-at-a-time. We need a way of talking about the sum of a bunch of random variables but without worrying about how many variables there are. Here we go:

\subsection{Calculating a Sum}

So, $k$ is a positive integer $(k \in \mathbb{N}_1)$, presumably greater than 1. So if there are $k$ random variables, the notation for the set of all them is $\{X_1,...,X_k\}$. However, it is even more compact to use matrix notation such that $\boldsymbol{X}=\{X_1,...,X_k\}$.

Now, $\bs{X}$ is a set of random variables in their entirety, without referencing any particular values those variables might generate. A set of particular values of these variables would be shown as $\bs{x}$ or $\{x_1,...,x_k\}$. In regular notation, the sum of these particular values would be:

\begin{equation}
x_S=\sum_{i=1}^{k}{x_i}
\end{equation}
Where\nopagebreak
\begin{conditions*}
k & The number of variables in $\{X_1,...,X_k\}$, $(k \in \mathbb{N}_1)$\\
x_S & The sum of all $k$ scores in $\{x_1,...,x_k\}$\\
x_i & A particular score generated by variable $X_i$\\
\end{conditions*}

\noindent In matrix notation, things are generally more compact:

\begin{equation*}
x_S=\boldsymbol{1'x}
\end{equation*}
Where\nopagebreak
\begin{conditions*}
\boldsymbol{x} & A $k \times 1$ vector of scores \{$x_1,...,x_k$\}\\
\boldsymbol{1} & A $k \times 1$ vector of ones\\
\end{conditions*}

The $\bs{1}$ may be a bit confusing. It is a column of ones that has the same length (number of elements) as $\bs{x}$. Therefore, $\bs{1'}$ is $\bs{1}$ \defword{transposed},\marginnote{To \defword{transpose} means to make all columns of a matrix into rows (or all rows into columns). Transposition is noted with a prime symbol ($\bs{'}$). If 
\[\bs{A}= \begin{bmatrix}
1 & 2 & 3\\
4 & 5 & 6\\
\end{bmatrix}\]
then
\[
\bs{A'}= \begin{bmatrix}
1 & 4\\
2 & 5\\
3 & 6\\
\end{bmatrix}
\]
}
 which is a row vector. A row vector multiplied by a column vector is the sum of the product of each analogous element in the pair of vectors. Suppose that $\bs{x}$ has a length of three. Thus,
\begin{equation*}
\bs{1'x}=\begin{bmatrix} 1\\ 1\\ 1 \end{bmatrix}'
\begin{bmatrix}x_1\\ x_2\\ x_3 \end{bmatrix} = 
\begin{bmatrix} 1&1&1\end{bmatrix}
\begin{bmatrix}x_1\\ x_2\\ x_3\end{bmatrix}
=1 \times x_1 + 1 \times x_2 + 1 \times x_3 = x_S
\end{equation*}

Let's do some calculations in R with a particular example. Suppose that there are three variables: $\boldsymbol{X}=\{X_1, X_2, X_3\}$. In R, we will create a vector of the names of variables in $\boldsymbol{X}$:
<<MakeXNames>>=
# Xnames = vector of variable names
Xnames <- c("X1","X2","X3")
@
\marginpar{The \texttt{c} function combines numbers (or other objects) into a vector.}
\noindent Now suppose that there are three particular scores: $\bs{x}=\{100,120,118\}$
<<MakeX,tidy=FALSE>>=
# x = vector of paricular scores from variables X1, X2, X3
x <- c(110,120,118)

# Applying Xnames to x (to make output easier to read)
names(x) <- Xnames 

# We can do both steps in a single line like so:
x <- c(110,120,118); names(x) <- Xnames 
@
\marginpar{The \texttt{names} function can return or set the names of the elements of a vector.}
\noindent The sum of these three scores can be calculated in a variety of ways. Here is the easiest:
<<SumX>>=
# x_S = The sum of scores x1, x2, and x3
x_S <- sum(x)
@
However, if we want to be matrix algebra masochists (and, apparently, at least one of us does!), we must first create a vector of ones that is the same length as $\bs{x}$. If all we cared about was this particular example, we could do so by hard coding a vector of three ones: \texttt{c(1,1,1)}. A better shortcut is to use the \texttt{rep} function,\marginpar{The \texttt{rep} function creates vectors of repeated elements. For example, \texttt{rep(5,3) is the same as \texttt{c(5,5,5)}.}} which creates a vector with the same element repeated a certain number of times. So, \texttt{rep(1,3)} creates a vector of three ones.

However, we want to use code that will be conveniently applied to a vector that is arbitrarily long. It would be no fun to have to count the vector length each time we run the code. The \texttt{length} function\marginpar{The \texttt{length} function returns the number of elements in a vector. For example, \texttt{length(c(3,3))} returns \texttt{2}.} tells us how many numbers are in a vector. That is, \texttt{length($\bs{x}$)} gives us the value of $k$, the number of variables. Using the \texttt{rep} and \texttt{length} functions together: 
<<Ones>>=
# A vector of ones the same length as x
ones <- rep(1,length(x))
@
Premultiplying any column vector by a row of ones produces the sum of the vector. You can think of any vector in R as a column vector. To make a vector into a row vector, you transpose it with the \texttt{t} function.\marginpar{The \texttt{t} function transposes a vector or matrix.} Matrix multiplication uses the matrix multiply operator \texttt{\%*\%}\marginpar{To multiply matrices $\bs{A}$ and $\bs{B}$ in R (assuming they are compatible), the operator \texttt{\%*\%} is used. $\bs{AB}$ in R is \texttt{A \%*\% B}.} like so:
<<MatrixSum>>=
# Calculating x_S with matrix algebra
x_S <- t(ones) %*% x
@
\noindent Either way that we calculate it, $x_S = \Sexpr{x_S}$.

\subsection{Calculating the Mean of a Sum}
The mean of $X_S$ is:
\begin{equation}
\mu_{S}=\sum_{i=1}^{k}{\mu_i}=\boldsymbol{1'\mu}
\end{equation}
Where\nopagebreak
\begin{conditions*}
\mu_S & The mean of $X_S$\\
\mu_i & The mean of $X_i$\\
\boldsymbol{\mu} & A $k \times 1$ vector of means of the variables in $\boldsymbol{X}$\\
\end{conditions*}
Suppose that the means of $X_1$, $X_2$, and $X_3$ are all 100. 
<<MakeMeans,tidy=FALSE>>=
# m = vector of means of X
m <- c(100,100,100)
names(m) <- Xnames
@
\noindent Again, the mean of $X_S$ $(\mu_S)$ can be calculated in two ways:
<<MeanC,tidy=FALSE>>=
# m_S = The mean of S
# The easy way
m_S <- sum(m)

# With matrix algebra
m_S <- t(ones) %*% m
@
\noindent Running this code, we can see that $\mu_S = \Sexpr{m_S}$.

\subsection{Calculating the Mean of a Weighted Sum}
The mean of a weighted sum is the weighted sum of the means. That is, if

\begin{equation}
x_S=\sum_{i=1}^{k}{w_i x_i}=\boldsymbol{w'x}
\end{equation}
Where\nopagebreak
\begin{conditions*}
w_i & The weight for $X_i$\\
\boldsymbol{w} & A $k \times 1$ vector of weights for each of the variables in $\boldsymbol{X}$\\
\end{conditions*}

\noindent then

\begin{equation}
\mu_S=\sum_{i=1}^{k}{w_i\mu_i}=\boldsymbol{w'\mu}
\end{equation}

Suppose that $\bs{w} = \{0.5,1,2\}$. That is, the weight for $X_1$ is 0.5, the weight for $X_2$ is 1, and the weight for $X_3$ is 2. We will continue to use the same values for $\bs{x}$ and $\bs{\mu}$ as before:\marginpar{Note that the calculation of $X_S$ and $\mu_S$ with matrix algebra is the same as it was with an equally weighted sum except that instead of premultiplying with a transposed vector of ones, we premultiply with a transposed vector of weights. In truth, an equally weighted sum is a special case of a weighted in which $\bs{w}$ consists entirely of ones.}
<<WeightedSum>>=
# w = The vector of weight for variables X1, X2, and X3
w = c(0.5,1,2)

# The easy way
x_S <- sum(w * x)
m_S <- sum(w * m)

# With matrix algebra
x_S <- t(w) %*% x
m_S <- t(w) %*% m
@
\noindent Running the code shows that $x_S = \Sexpr{x_S}$ and that $\mu_S = \Sexpr{m_S}$.\marginpar{In R, the multiplication operator (\texttt{*}) multiplies analogous elements of vectors and matrices. In the example, \texttt{w * s} is\\ $\{0.5 * 15, 1 * 15, 2 * 15\}$}

\section{The Variance of the Sum Is the Sum of the Covariance Matrix}

If variables are uncorrelated, the variance of their sum is the sum of their variances. However, this is not true when variables are substantially correlated. The formula for the variance of a sum looks more complex than it is. It is just the sum of the covariance matrix.\marginpar{Unfortunately, the notation for a covariance matrix is a bold capital sigma $\bs{\Sigma}$, which is easily confused with the summation symbol, which is generally larger and not bold: $\sum$.}

\begin{equation}\label{eq:VarianceOfASum}
\sigma_{X_S}^2=\sum_{i=1}^{k}{\sum_{j=1}^{k}{\sigma_{ij}}}=\boldsymbol{1'\Sigma 1}
\end{equation}
Where\nopagebreak
\begin{conditions*}
\sigma_{X_S}^2 & The variance of $X_S$\\
\sigma_{ij} & The covariance between $X_i$ and $X_j$ ($\sigma_{ij}=\sigma_i^2$ if $i=j$)\\
\boldsymbol{\Sigma} & The $k \times k$ covariance matrix of all the variables in $\boldsymbol{X}$
\end{conditions*}

Suppose that the standard deviations of $X_1$, $X_2$, and $X_3$ are all 15. Thus, $\sigma=\{15,15,15\}$. The correlations among the three variables are shown in matrix $\bs{R}$:\marginpar{The symbol for a sample correlation is the Roman lowercase $r$ and a matrix of such correlations is an uppercase $\bs{R}$. Therefore, the population correlation coefficient is a Greek lowercase rho: $\rho$. This, unfortunately means that a matrix of correlations should be an uppercase rho: $\bs{\Rho}$. Somehow, statisticians are okay with $\rho$ looking a lot like an italicized Roman letter \textit{p}. However, using an uppercase rho for a correlation matrix is too crazy even for statisticians! You know, hobgoblins of little minds and all...}
\[
\bs{R}=
\begin{bmatrix}
<<DisplayR, echo = FALSE,results='asis'>>=
# R = correlation matrix of variables in X
R <- rbind(
  c(1,0.5,0.6),#
  c(0.5,1,0.7),#
  c(0.6,0.7,1))#

# A different way to specify R
R <- matrix(data=c(1,0.5,0.6, 0.5,1,0.7,  0.6,0.7,1),nrow=3,ncol=3,byrow=TRUE)
rownames(R) <- colnames(R) <- Xnames
cat(bmatrix(R))
@
\end{bmatrix}
\]
\noindent The covariance matrix $\bs{\Sigma}$ can be computed from the correlation matrix and the standard deviations like so:
\[
\sigma_{ij} = \sigma_{i} \sigma_{j} \rho_{ij}
\]
Where\nopagebreak
\begin{conditions*}
\sigma_{ij} & The covariance between $X_i$ and $X_j$\\
\rho_{ij} & The correlation between $X_i$ and $X_j$\\
\sigma_{i} & The standard deviation of variable $X_i$\\
\sigma_{j} & The standard deviation of variable $X_j$\\
\end{conditions*}

\noindent Unfortunately, computing a covariance matrix like this in a computer program is inelegant because we have to make use of looping:
<<LoopingCM>>=
# R = Correlation matrix of variables in X
R <- matrix(c(1,0.5,0.6,
              0.5,1,0.7,
              0.6,0.7,1),nrow=3)
rownames(R) <- colnames(R) <- Xnames #Apply names

# s = The standard deviations of variables in X
s <- c(15,15,15)

# k = The number of variables
k <- length(s)

# CM = Covariance Matrix
#mat.or.vec initializes a matrix of the right size
CM <- mat.or.vec(k, k)
rownames(CM) <- colnames(CM) <- Xnames

for (i in seq(1,k)){
  for (j in seq(1,k)){
    CM[i,j]=s[i]*s[j]*R[i,j]
  }
}
@

To calculate the covariance matrix in matrix notation, we can make use of the $\mathtt{diag}$ function. The $\mathtt{diag}$ function has two jobs: to turn a matrix into a vector and a vector into a matrix. Specifically, it takes the diagonal of a matrix and turns it into a vector. That is, if
\[
A_{k,k} =
 \begin{bmatrix}
  a_{1,1} & a_{1,2} & \cdots & a_{1,k} \\
  a_{2,1} & a_{2,2} & \cdots & a_{2,k} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  a_{k,1} & a_{k,2} & \cdots & a_{k,k}
 \end{bmatrix}
\]

\noindent Then $\mathtt{diag}(\bs{A}) = \{a_{1,1},a_{2,2},...,a_{k,k}\}$.

\noindent It also replaces the diagonal of a $k \times k$ matrix of zeros with a $k \times 1$ vector, like so:
\[
\mathtt{diag}(\bs{a}) = 
\begin{bmatrix}
  a_{1} & 0 & \cdots & 0 \\
  0 & a_{2} & \cdots & 0 \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  0 & 0 & \cdots & a_{k}
 \end{bmatrix}
\]

\noindent This is useful for calculating the covariance matrix (among many other statistics):

\begin{align*}
\bs{\Sigma}&=\mathtt{diag}(\bs{\sigma}) \bs{R}\, \mathtt{diag}(\bs{\sigma})\\ 
&=\begin{bmatrix}
  \sigma_{1} & 0 & \cdots & 0 \\
  0 & \sigma_{2} & \cdots & 0 \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  0 & 0 & \cdots & \sigma_{k}
 \end{bmatrix}
  \begin{bmatrix}
  1 & \rho_{1,2} & \cdots & \rho_{1,k} \\
  \rho_{2,1} & 1 & \cdots & \rho_{2,k} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \rho_{k,1} & \rho_{k,2} & \cdots & 1
 \end{bmatrix}
\begin{bmatrix}
  \sigma_{1} & 0 & \cdots & 0 \\
  0 & \sigma_{2} & \cdots & 0 \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  0 & 0 & \cdots & \sigma_{k}
 \end{bmatrix}\\
  &=  \begin{bmatrix}
  \sigma_{1}^2 & \sigma_{1}\sigma_{2}\rho_{1,2} & \cdots & \sigma_{1}\sigma_{k}\rho_{1,k} \\
  \sigma_{2}\sigma_{1}\rho_{2,1} & \sigma_{2}^2 & \cdots & \sigma_{2}\sigma_{k}\rho_{2 k} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \sigma_{k}\sigma_{1}\rho_{k,1} & \sigma_{k}\sigma_{2}\rho_{k,2} & \cdots & \sigma_{k}^2
 \end{bmatrix}\\
   &=  \begin{bmatrix}
  \sigma_{1}^2 & \sigma_{1,2} & \cdots & \sigma_{1,k} \\
  \sigma_{2,1} & \sigma_{2}^2 & \cdots & \sigma_{2,k} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \sigma_{k,1} & \sigma_{k,2} & \cdots & \sigma_{k}^2
 \end{bmatrix}
\end{align*}
Where\nopagebreak
\begin{conditions*}
\bs{\Sigma} & The covariance matrix of variables in $\bs{X}$\\
\bs{\sigma} & The standard deviations of variables in  $\bs{X}$\\
\bs{R} & The correlation matrix of variables in $\bs{X}$\\
\end{conditions*}

\noindent So this is where matrix algebra starts to shine. We do not need to initialize the variable that contains the covariance matrix, or calculate $k$, or do any looping. R will even automatically apply the names from the correlation matrix to the covariance matrix.

<<CovarianceMatrix>>=
# CM = Covariance matrix of variables in X
CM <- diag(s) %*% R %*% diag(s)
@

\noindent Beautiful!

\noindent Running the code, we get:
\[
\bs{\Sigma}=
\begin{bmatrix}
<<DisplayCM,echo = FALSE,results='asis'>>=
cat(bmatrix(CM))
@
\end{bmatrix}
\]
To calculate the variance of $X_S$ if it is an unweighted sum we apply Equation~\ref{eq:VarianceOfASum}:

<<CalculatingVarianceofComposite>>=
# var_S = The variance of X_S
var_S <- sum(CM)

# With matrix algebra
var_S <- t(ones) %*% CM %*% ones
#
@
\noindent Here we see that $\sigma_X^2=\Sexpr{var_S}$

\section{Calculating the Variance of a Weighted Sum}

\noindent If $X_S$ is a weighted sum, its variance is the weighted sum of the covariance matrix.
\begin{equation}\label{eq:VarianceOfAWeightedSum}
\sigma_S^2=\sum_{i=1}^{k}{\sum_{j=1}^{k}{w_i w_j \sigma_{ij}}}=\boldsymbol{w'\Sigma w}
\end{equation}

Continuing with the same variables in our example, we see that things are starting to become clumbsy and ugly without matrix algebra:
<<CalculatingVariance>>=
# First we initialize var_S as 0
var_S <- 0
# Now we loop through k rows and k columns of CM
for (i in seq(1,k)){
  for (j in seq(1,k)){
    var_S <- var_S + w[i]*w[j]*CM[i,j]
  }
}


# With matrix algebra, 
# this all happens with a single line of code:
var_S <- t(w) %*% CM %*% w
#

# If we don't need to know the covariance matrix,
# we can skip its calculation:
var_S <- t(w * s) %*% R %*% (w * s)
@
\noindent All three methods give the same answer: $\sigma_X^2 = \Sexpr{var_S}$

\section{Calculating a Composite Score}

We now have all of the information needed to make a composite score. 

\begin{equation}
c = \frac{x_S-\mu_S}{\sigma_{X_S}}\sigma_C + \mu_C
\end{equation}
Where\nopagebreak
\begin{conditions*}
c & The particular score from composite variable $C$\\
x_S & The sum of all $k$ scores in $\{x_1,...,x_k\}$\\
\sigma_{X_S} & The standard deviation of $X_S$\\
\sigma_C & The standard deviation of $C$\\
\mu_C & The mean of $C$\\
\end{conditions*}

Continuing with our example, suppose that $\mu_C=100$ and $\sigma_C=15$.
<<CompositeGeneral>>=
# m_C = Composite mean
m_C <- 100
# s_C = Composite standard deviation
s_C <- 15
# c = The composite score
c <- ((x_S - m_S) / sqrt(var_S)) * s_C + m_C
@

In clinical practice, the most common kind of composite score is an equally weighted composite consisting of scores with the same mean and standard deviation. Sometimes scores are weighted by the degree to which they correlate with the construct the composite is intended to measure. Other weighting schemes are also possible. In most cases, it makes sense to first convert all scores to z-scores and then weight the z-scores. Failing to convert the scores to a common metric such as z-scores will result in an implicit weighting by standard deviations. That is, the score with the largest standard deviation will have the most weight in the composite score. Converting to z-scores first will equalize the each score's influence on the composite score. We can convert all the scores in $\bs{x}$ to z-scores like so:
\[
\bs{z} = (\bs{x}-\bs{\mu})/\bs{\sigma}
\]
\marginpar{If element-wise division seems inelegant or somehow undignified, this little bit of trickery also works but seems like overkill to me:
\[
\bs{z} = \texttt{diag}(\texttt{diag}(\bs{\sigma})^{-1}(\bs{x}-\bs{\mu}))
\]}

The nice thing about z-scores is that their means are zeros, their standard deviations are ones, and their covariances are correlations. Thus, the formula for a weighted composite score consisting of z-scores is fairly simple, especially if the composite score is a z-score itself:
\[
c = \frac{\bs{w'z}}{\sqrt{\bs{w'Rw}}}
\]
Continuing with our example, computing a composite score with $X_1=\Sexpr{x[1]}$, $X_2=\Sexpr{x[2]}$, and $X_3=\Sexpr{x[3]}$:\marginpar{The \texttt{round} function rounds to the nearest integer by default but you can round to any number of digits you wish. For example, rounding to 2 significant digits (i.e., the hundredths place), would be \texttt{round(x,2)}.}

<<CalculateWeightedComposite>>=
# z = The scores in x converted to z-scores
z <- (x - m)/ s

# c = The composite score
c <- sum(w * z) / sqrt(sum(t(w) %*% R %*% w))

# If we wish, we can convert c to an index score metric
c <- c * 15 +100

# With index scores we round to the nearest integer
c <- round(c)
@

Here we see, that the weighted composite score $c = \Sexpr{c}$. If we had wanted an equally weighted composite score, the \texttt{w} vector would be set to equal ones. If we had done so, $c$ would have been $\Sexpr{round(100+15*sum(ones * z) / sqrt(sum(t(ones) %*% R %*% w)))}$
instead of $\Sexpr{c}$.

\section{The reliability coefficient of a composite}

Calculating the reliability coefficient of a composite score is much easier than it might appear at first. Remember that reliability is the ratio of a score's true score variance to its total variance:

\[
r_{XX} = \frac{\sigma_T^2}{\sigma_X^2}
\]

We know what the total variance of a composite score is: the sum of the covariance matrix of the composite's component scores. The variance of the composite's true scores is also the the sum of the covariance matrix of the component scores except that the diagonal of the covariance matrix is multiplied by the reliability coefficients of each of the component scores like so:

\[
\sigma_T^2=\bs{1}'\begin{bmatrix}
  {\color{red}r_{11}}\sigma_{1}^2 & \sigma_{1,2} & \cdots & \sigma_{1,k} \\
  \sigma_{2,1} & {\color{red}r_{22}}\sigma_{2}^2 & \cdots & \sigma_{2 k} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \sigma_{k,1} & \sigma_{k,2} & \cdots & {\color{red}r_{kk}}\sigma_{k}^2
 \end{bmatrix}\bs{1}
\]

<<CalculateReliability>>=
# r_XX = Reliability coefficients for X1, X2, and X3
r_XX <- c(0.88,0.80,0.76)

# CM_T = Covariance matrix of true scores
CM_T <- CM
# Replace diagonal with true score variances
diag(CM_T) <- diag(CM) * r_XX

# r_C = Reliability coefficient of composite score
r_C <- sum(CM_T) / sum(CM)
@

\section{Composite Scores and Their Correlations}

If the population correlations among all of the components are known, it is possible to calculate the correlations among composite scores made from these components. Suppose that Composite $A$ is calculated from the sum of two component tests, $A_1$ and $A_2$. Composite $B$ is calculated from the sum of $B_1$ and $B_2$. Suppose that the correlation matrix for the four components is:

<<FourComponentCorrelation,results='asis',echo=FALSE>>=
R4 <- rbind(
  c(1,0.3,0.35,0.4), 
	c(0.3,1,0.42,0.48), 
	c(NA,NA,1,0.56), 
	c(NA,NA,0.56,1))
R4Names <- c("$A_1$", "$A_2$", "$B_1$", "$B_2$")
rownames(R4) <-colnames(R4) <- R4Names
# R4<-xtable(R4)
# print(R4, sanitize.text.function = function(x){x})
cellTex <- matrix(rep("", NROW(R4) * NCOL(R4)), nrow = NROW(R4))
cellTex[c(1,2), c(1,2)] <- "cellcolor{firebrick}"
cellTex[c(3,4), c(3,4)] <- "cellcolor{royalblue}"
cellTex[c(1,2), c(3,4)] <- "cellcolor{xPurple}"
latex(R4, file = "", cellTexCmds = cellTex, numeric.dollar = FALSE,colnamesTexCmd="bfseries",rownamesTexCmd="bfseries",na.blank=TRUE,first.hline.double=FALSE,table.env=FALSE,align="|l|c|c|c|c|",title="")
@

The correlation between Composite $A$ and Composite $B$ is calculating by adding up the numbers is all three shaded regions of the correlation matrix and then dividing the sum of ``between'' correlations in purple by the geometric mean of the sums from the ``within'' correlations in the blue and red regions:
\[r_{AB}=\frac{\colorbox{xPurple}{Sum of Correlations Between $A$ and $B$}}{\sqrt{\colorbox{firebrick}{Sum of Correlations Within $A$}\times\colorbox{royalblue}{Sum of Correlations Within $B$}}}\]
% 
% 
% <<CompositeCalculations,echo=TRUE,dev='pdf',tidy=TRUE>>=
% # Xnames = vector of component names
% Xnames <- c("X1","X2","X3")
% 
% # m = vector of means of components
% m <- c(100,100,100)
% 
% # x_S = vector of standard deviations of components
% x_S <- c(15,15,15)
% 
% # w = vector of weights of components
% w <- c(0.5,1,2)
% 
% # R = correlation matrix of components
% R <- rbind(
%   c(1,0.5,0.6),#
%   c(0.5,1,0.7),#
%   c(0.6,0.7,1))#
% 
% # r = vector of reliability coefficients
% r <- c(0.88,0.91,0.92)
% 
% # x = vector of component scores
% x <- c(110,120,118)
% 
% # Apply component names to vectors and correlation matrix
% names(m)<-names(s)<-names(w)<-names(x)<-Xnames
% rownames(R)<-colnames(R)<-names(r)<-Xnames
% 
% # z = vector of component scores transformed to z-scores
% z <- (x - m) / s
% 
% # wSumz = weighted sum of z-scores
% wSumz<-sum(w * z)
% 
% # v_wSumz = variance of weighted sum of z-scores
% v_wSumz<-t(w) %*% R %*% w
% 
% # s_wSumz = standard deviation of weighted sum of z-scores
% s_wSumz<-sqrt(v_wSumz)
% 
% # z_C = composite score as a z-score
% z_C <- wSumz / s_wSumz
% 
% # m_C = mean of composite score
% m_C <- 100
% 
% # s_C = standard deviation of composite score
% s_C <- 15
% 
% # C = composite score
% C <- z_C * s_C + m_C
% 
% # r_C = reliability of composite score
% r_C <- (v_wSumz + t(r - 1) %*% (w^2)) / v_wSumz
% 
% # se_C = standard error of composite score
% se_C <- s_C * sqrt(1 - r_C)
% 
% # see_C = standard error of the estimate of the composite score
% see_C <- s_C * sqrt(r_C * (1 - r_C))
% 
% # p_CI = Percent Confidence
% p_CI <- 95/100
% 
% # z_CI = z-score associated with CI
% z_CI <- qnorm(1 - (1 - p_CI) / 2)
% 
% # ObservedCenteredCI_C = observed-score centered confidence interval of the composite
% ObservedCenteredCI_C <- C + z_CI * se_C * c(-1,1)
% 
% # estT_C = estimated true score of the composite
% estT_C <- (C - m_C) * r_C + m_C
% 
% # TrueCenteredCI_C = true-score centered confidence interval of the composite
% TrueCenteredCI_C <- estT_C + z_CI * see_C * c(-1,1)
% 
% @
